</h1>
<div align="center">
    <h1>Awesome Energy LLM papers</h1>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg"/></a>
</div>

<p align="center">
<font face="Èªë‰Ωì" color=orange size=5"> An Awesome Collection for energy LLM papers  </font>
</p>
<p align="center">
  <a href="https://github.com/chenweilong915/awesome_energy_LLM/stargazers"> <img src="https://img.shields.io/github/stars/chenweilong915/awesome_energy_LLM.svg?style=popout-square" alt="GitHub stars"></a>
  <a href="https://github.com/chenweilong915/awesome_energy_LLM/issues"> <img src="https://img.shields.io/github/issues/chenweilong915/awesome_energy_LLM.svg?style=popout-square" alt="GitHub issues"></a>
  <a href="https://github.com/chenweilong915/awesome_energy_LLM/forks"> <img src="https://img.shields.io/github/forks/chenweilong915/awesome_energy_LLM.svg?style=popout-square" alt="GitHub forks"></a>
</p>



This repository collects **papers**, **code**, and **resources** related to applying **Large Language Models (LLMs)** in the **energy domain**. The goal is to help researchers, engineers, and students stay up-to-date with recent advances, implementation practices, and emerging trends in this interdisciplinary field.

---

## üîç Scope

We focus on applications of LLMs in areas including but not limited to:

* **Load forecasting**
* **Predicting**
* **Optimal Power Flow (OPF)**
* **Deep Reinforcement Learning related**
* **‚≠êÔ∏èBonus‚≠êÔ∏è: LLM-based Time Series Analysis**

## üìÑ Papers

### Survey

- [[Arxiv](https://arxiv.org/pdf/2506.06359)] From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins. `2025.06`


### Forecasting and Predicting
- [[Applied Energy](https://www.sciencedirect.com/science/article/pii/S0306261925006956)] MMGPT4LF: Leveraging an optimized pre-trained GPT-2 model with multi-modal cross-attention for load forecasting. `2025.08`

- [[Expert Systems with Applications](https://www.sciencedirect.com/science/article/pii/S0957417425010589)] Prompting large language model for multi-location multi-step zero-shot wind power forecasting.`2025.06`

- [[arXiv](https://arxiv.org/abs/2503.06216)] A Novel Distributed PV Power Forecasting Approach Based on Time-LLM. `2025.05`

- [[Applied Energy](https://www.sciencedirect.com/science/article/pii/S0306261925001023)] Implementing a provincial-level universal daily industrial carbon emissions prediction by fine-tuning the large language model. `2025.04`

- [[Applied Energy](https://www.sciencedirect.com/science/article/pii/S0306261925000881)] Toward Large Energy Models: A comparative study of Transformers‚Äô efficacy for energy forecasting. `2025.04`

- [[IEEE TPS](https://ieeexplore.ieee.org/abstract/document/10917006)] Empower Pre-Trained Large Language Models for Building-Level Load Forecasting. `2025.03`

- [[arXiv](https://arxiv.org/abs/2502.16896)] Zero-shot Load Forecasting for Integrated Energy Systems: A Large Language Model-based Framework with Multi-task Learning. `2025.02`

- [[Applied Energy](https://www.sciencedirect.com/science/article/pii/S0306261924017616)] Domain-specific large language models for fault diagnosis of heating, ventilation, and air conditioning systems by labeled-data-supervised fine-tuning. `2025.01`

- [[Applied Energy](https://www.sciencedirect.com/science/article/pii/S0306261924023572)] TimeGPT in load forecasting: A large time series model perspective. `2025.01`

- [[Applied Energy](https://www.sciencedirect.com/science/article/pii/S030626192402049X)] Applying fine-tuned LLMs for reducing data needs in load profile analysis. `2025.01`

- [[Applied Energy](https://www.sciencedirect.com/science/article/pii/S030626192401417X)] STELLM: Spatio-temporal enhanced pre-trained large language model for wind speed forecasting. `2024.11`

- [[arXiv](https://arxiv.org/abs/2411.11350)] Zero-Shot Load Forecasting with Large Language Models. `2024.11`

- [[arXiv](https://arxiv.org/abs/2406.11336)] A General Framework for Load Forecasting based on Pre-trained Large Language Model. `2024.09`

- [[Applied Energy](https://www.sciencedirect.com/science/article/pii/S0306261924008146)] EPlus-LLM: A large language model-based computing platform for automated building energy modeling. `2024.08`

- [[TechRxiv](https://www.techrxiv.org/doi/full/10.36227/techrxiv.170475236.64005369)] LFLLM: A Large Language Model for Load Forecasting. `2024.01`

### Optimal Power Flow (OPF)

- [[Scientific reports](https://www.nature.com/articles/s41598-025-91940-x)] A large language model for advanced power dispatch. `2025.03`

- [[arXiv](https://arxiv.org/abs/2501.07639)] SafePowerGraph-LLM: Novel Power Grid Graph Embedding and Optimization with Large Language Models. `2025.01`

- [[IEEE TPS](https://ieeexplore.ieee.org/document/10339881)] Real-Time Optimal Power Flow With Linguistic Stipulations: Integrating GPT-Agent and Deep Reinforcement Learning. `2023.11`



### Deep Reinforcement Learning Related

- [[Applied Energy](https://www.sciencedirect.com/science/article/pii/S0306261925005707)] Adaptive infinite-horizon control of hybrid EV/FCEV charging hubs: A large-model based deep reinforcement learning approach. `2025.07`

- [[IEEE TPS](https://ieeexplore.ieee.org/document/10339881)] Real-Time Optimal Power Flow With Linguistic Stipulations: Integrating GPT-Agent and Deep Reinforcement Learning. `2023.11`




### other topic
- [[IEEE TSG](https://ieeexplore.ieee.org/document/10675341)] A Large Language Model for Determining Partial Tripping of Distributed Energy Resources. `2025.01`

- [[IEEE TSG](https://ieeexplore.ieee.org/document/10663471)] Large Language Model for Smart Inverter Cyber-Attack Detection via Textual Analysis of Volt/VAR Commands. `2024.09`

- [[Arxiv](https://arxiv.org/pdf/2408.03847)] GAIA -- A Large Language Model for Advanced Power Dispatch. `2024.08`

- [[IEEE TSG](https://ieeexplore.ieee.org/document/10459250)] Applying Large Language Models to Power Systems: Potential Security Threats. `2024.03`

- [[Arxiv](https://arxiv.org/pdf/2408.03847)] GAIA -- A Large Language Model for Advanced Power Dispatch. `2024.08`


### ‚≠êÔ∏èBONUS‚≠êÔ∏è: LLM-based Time Series Analysis

#### survey

- [[arXiv](https://arxiv.org/abs/2503.13709)] Multi-modal Time Series Analysis: A Tutorial and Survey. `2025.03` [`code`](https://github.com/UConn-DSIS/Multi-modal-Time-Series-Analysis)

- [[arXiv](https://www.techrxiv.org/doi/full/10.36227/techrxiv.174317777.72957387)] Beyond Numbers: A Survey of Time Series Analysis in the Era of Multimodal LLMs. `2025.03` [`code`](https://github.com/mllm-ts/Awesome-Multimodal-LLMs-Time-Series
#### benchmark

- [[arXiv](https://arxiv.org/abs/2405.13522)] Intervention-Aware Forecasting: Breaking Historical Limits from a System Perspective. `2025.05` [`code1`](https://github.com/VEWOXIC/Universal-Cross-Modal-Time-Series-Forecasting-Pipeline) [code2](https://github.com/VEWOXIC/TGTSF) [dataset](https://github.com/VEWOXIC/Weather-Captioned) [ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑](https://mp.weixin.qq.com/s/w5xIAr11M6qmwMh7tDw92w)

- [[arXiv](https://arxiv.org/abs/2503.01875)] Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement. `2025.02` [`code`](https://huggingface.co/Time-QA)

- [[arXiv](https://arxiv.org/abs/2410.18959)] Context is Key: A Benchmark for Forecasting with Essential Textual Information. `2025.02` [`code`](https://github.com/ServiceNow/context-is-key-forecasting)


#### technique paper
- [[arXiv](https://arxiv.org/abs/2506.08641)] Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers. `2025.07` 

- [[arXiv](https://arxiv.org/abs/2506.01290)] TSRating: Rating Quality of Diverse Time Series Data by Meta-learning from LLM Judgment. `2025.06` 

- [[arXiv](https://arxiv.org/abs/2506.10630)] Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs. `2025.06` 

- [[arXiv](https://arxiv.org/abs/2505.02138)] Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation. `2025.05` 

- [[VLDB 25](https://arxiv.org/abs/2412.03104)] ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced Understanding and Reasoning. `2025.04` [`code`](https://github.com/NetManAIOps/ChatTS) [`ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑`](https://mp.weixin.qq.com/s/6a9LCaaZPemXHSZOmiik0g)

- [[arXiv](https://arxiv.org/abs/2502.01477)] Position: Empowering Time Series Reasoning with Multimodal LLMs. `2025.02` [`code`](https://github.com/ServiceNow/context-is-key-forecasting) [`ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑`](https://mp.weixin.qq.com/s/PhSwaqa102YCzOwbPxLWWQ)

- [[ICLR 25](https://openreview.net/pdf?id=uCqxDfLYrB)] Towards Neural Scaling Laws for Time Series Foundation Models. `2025.01` [`code`](https://github.com/Qingrenn/TSFM-ScalingLaws) [`ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑`](https://mp.weixin.qq.com/s/tSN2gSajYTpS9cDcGlmJlw)

- [[WWW 25](https://openreview.net/forum?id=dFapOK8Rhb)] Exploiting Language Power for Time Series Forecasting with Exogenous Variables (EXOLLM). `2025.01` [`code`](https://github.com/h505023992/ExoLLM) [`ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑`](https://mp.weixin.qq.com/s/oOuFFzsaGZYThHS3fvZHyA)

- [[mdpi](https://www.mdpi.com/2073-8994/17/3/401)] TS-HTFA: Advancing Time-Series Forecasting via Hierarchical Text-Free Alignment with Large Language Models. `2025.01` 

- [[TMLR](https://arxiv.org/abs/2410.04803)] Chronos: Learning the Language of Time Series. `2024.11` [`code`](https://github.com/amazon-science/chronos-forecasting) [`blog`](https://www.amazon.science/blog/adapting-language-model-architectures-for-time-series-forecasting)

- [[ICLR 2025](https://arxiv.org/abs/2410.04803)] Timer-XL: Long-Context Transformers for Unified Time Series Forecasting. `2024.11` [`code`](https://github.com/thuml/Timer-XL) [`ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑`](https://mp.weixin.qq.com/s/IFqysOWo1prdjeBpCiNXBg)

- [[NeurIPS 2024](https://arxiv.org/pdf/2402.02370)] AutoTimes: Autoregressive Time Series Forecasters via Large Language Models. `2024.10`  [`code`](https://github.com/thuml/AutoTimes) [`ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑`](https://mp.weixin.qq.com/s/BiMptoer_7xTBaVgHPI9xg)

- [[NeurIPS 2024](https://proceedings.neurips.cc/paper_files/paper/2024/file/6ed5bf446f59e2c6646d23058c86424b-Paper-Conference.pdf)] Are Language Models Actually Useful for Time Series Forecasting? `2024.10` [`code`](https://github.com/BennyTMT/LLMsForTimeSeries)

- [[ArXiv](https://arxiv.org/abs/2410.10469)] Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts. `2024.10` 

- [[WWW 2024](https://arxiv.org/pdf/2402.02370)] UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting. `2024.05`  [`code`](https://github.com/liuxu77/UniTime)

- [[ArXiv](https://arxiv.org/abs/2402.02592)] Unified Training of Universal Time Series Forecasting Transformers (Moirai). `2024.05` [`code`](https://github.com/SalesforceAIResearch/uni2ts)



- [[ICLR 2024](https://arxiv.org/abs/2402.03885)] MOMENT: A Family of Open Time-series Foundation Models. `2024.05` [`code`](https://huggingface.co/AutonLab/MOMENT-1-large)

- [[ICLR 2024](https://arxiv.org/abs/2310.04948)] TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. `2024.05` [`code`](https://github.com/DC-research/TEMPO)

- [[ICML 2024](https://arxiv.org/abs/2310.10688)] A decoder-only foundation model for time-series forecasting. `2024.04` [`code`](https://github.com/google-research/timesfm)

- [[arXiv](https://arxiv.org/abs/2308.08241)] TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. `2024.02`

- [[ICLR 2024](https://arxiv.org/abs/2310.01728)] Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. `2024.01` [`code`](https://github.com/KimMeen/Time-LLM)

- [[NeurIPS 2023](https://proceedings.neurips.cc/paper_files/paper/2023/file/86c17de05579cde52025f9984e6e2ebb-Paper-Conference.pdf)] One Fits All:
Power General Time Series Analysis by Pretrained LM (FPT). `2023.10` [`code`](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All)

- [[NeurIPS 2023](https://proceedings.neurips.cc/paper_files/paper/2023/file/3eb7ca52e8207697361b2c0fb3926511-Paper-Conference.pdf)] Large Language Models Are Zero-Shot Time Series Forecasters (LLMTime).  `2023.10` [`code`](https://github.com/ngruver/llmtime)

- [[TKDE](https://arxiv.org/pdf/2210.08964)] PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting. `2022.09`



---

## üìå Other related repo

[`Awesome-Multimodal-LLMs-Time-Series`](https://github.com/mllm-ts/Awesome-Multimodal-LLMs-Time-Series)

[`Multi-modal-Time-Series-Analysis`](https://github.com/UConn-DSIS/Multi-modal-Time-Series-Analysis)




---

## üöß Contributions

Contributions are welcome! If you have papers, code, or datasets to share, feel free to open a pull request or submit an issue.

---

## üì¨ Contact

Maintained by [Weilong Chen](mailto:chenweilong921@gmail.com).
If you find this project useful, feel free to ‚≠êÔ∏è star and share it.


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=chenweilong915/awesome_energy_LLM&type=Date)](https://www.star-history.com/#chenweilong915/awesome_energy_LLM&Date)